{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea512ed-f4fa-4559-a947-603122fe7cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 12:15:10,825 - INFO - Načítání CSV souboru...\n",
      "2025-04-23 12:15:11,107 - INFO - Začínám scrapování...\n",
      "Dávka 1/207: 100%|██████████| 1000/1000 [00:22<00:00, 44.24it/s]\n",
      "Dávka 2/207:  75%|███████▌  | 750/1000 [00:15<00:04, 55.09it/s]2025-04-23 12:15:48,946 - ERROR - Chyba při scrapování https://www.autodoplnky-obchod.cz/autopotahy-vw-transporter-t6-2015-predni-3-mista-/: Document is empty\n",
      "Dávka 2/207: 100%|██████████| 1000/1000 [00:19<00:00, 50.92it/s]\n",
      "Dávka 3/207: 100%|██████████| 1000/1000 [00:19<00:00, 51.54it/s]\n",
      "Dávka 4/207: 100%|██████████| 1000/1000 [00:21<00:00, 46.93it/s]\n",
      "Dávka 5/207: 100%|██████████| 1000/1000 [00:23<00:00, 43.16it/s]\n",
      "Dávka 6/207: 100%|██████████| 1000/1000 [00:26<00:00, 38.21it/s]\n",
      "Dávka 7/207: 100%|██████████| 1000/1000 [00:27<00:00, 36.80it/s]\n",
      "Dávka 8/207: 100%|██████████| 1000/1000 [00:32<00:00, 30.84it/s]\n",
      "Dávka 9/207: 100%|██████████| 1000/1000 [00:37<00:00, 26.89it/s]\n",
      "Dávka 10/207: 100%|██████████| 1000/1000 [00:41<00:00, 24.16it/s]\n",
      "2025-04-23 12:19:42,663 - INFO - Průběžné výsledky uloženy do scraped/temp_results_9000.csv\n",
      "Dávka 11/207: 100%|██████████| 1000/1000 [00:45<00:00, 21.75it/s]\n",
      "Dávka 12/207: 100%|██████████| 1000/1000 [00:51<00:00, 19.48it/s]\n",
      "Dávka 13/207: 100%|██████████| 1000/1000 [01:02<00:00, 15.91it/s]\n",
      "Dávka 14/207: 100%|██████████| 1000/1000 [01:10<00:00, 14.16it/s]\n",
      "Dávka 15/207: 100%|██████████| 1000/1000 [01:17<00:00, 12.88it/s]\n",
      "Dávka 16/207: 100%|██████████| 1000/1000 [01:19<00:00, 12.55it/s]\n",
      "Dávka 17/207: 100%|██████████| 1000/1000 [01:31<00:00, 10.96it/s]\n",
      "Dávka 18/207: 100%|██████████| 1000/1000 [01:29<00:00, 11.23it/s]\n",
      "Dávka 19/207: 100%|██████████| 1000/1000 [01:43<00:00,  9.66it/s]\n",
      "Dávka 20/207: 100%|██████████| 1000/1000 [02:00<00:00,  8.30it/s]\n",
      "2025-04-23 12:32:56,679 - INFO - Průběžné výsledky uloženy do scraped/temp_results_19000.csv\n",
      "Dávka 21/207: 100%|██████████| 1000/1000 [02:08<00:00,  7.80it/s]\n",
      "Dávka 22/207: 100%|██████████| 1000/1000 [02:21<00:00,  7.06it/s]\n",
      "Dávka 23/207: 100%|██████████| 1000/1000 [02:39<00:00,  6.25it/s]\n",
      "Dávka 24/207: 100%|██████████| 1000/1000 [02:46<00:00,  6.00it/s]\n",
      "Dávka 25/207: 100%|██████████| 1000/1000 [03:20<00:00,  4.98it/s]\n",
      "Dávka 26/207: 100%|██████████| 1000/1000 [03:19<00:00,  5.02it/s]\n",
      "Dávka 27/207:  52%|█████▏    | 517/1000 [01:43<01:41,  4.76it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import html\n",
    "import concurrent.futures\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Nastavení loggeru\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Funkce pro scrapování jedné URL adresy\n",
    "def scrape_url(url):\n",
    "    try:\n",
    "        # Přidání timeout pro zabránění zablokování\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return url, None, None\n",
    "        \n",
    "        # Parsování HTML\n",
    "        tree = html.fromstring(response.content)\n",
    "        \n",
    "        # Extrakce dat podle prvního XPath\n",
    "        xpath_result_1 = tree.xpath(\"//span[@itemprop='itemListElement']/a/span[@itemprop='name']/text()\")\n",
    "        result_1 = xpath_result_1[0] if xpath_result_1 else None\n",
    "        \n",
    "        # Extrakce dat podle druhého XPath (regex)\n",
    "        page_type = None\n",
    "        match = re.search(r'\"pageType\"\\s*:\\s*\"(.*?)\"', response.text)\n",
    "        if match:\n",
    "            page_type = match.group(1)\n",
    "        \n",
    "        return url, result_1, page_type\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chyba při scrapování {url}: {str(e)}\")\n",
    "        return url, None, None\n",
    "\n",
    "# Hlavní funkce\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Vytvoření složky pro výstup, pokud neexistuje\n",
    "    if not os.path.exists(\"scraped\"):\n",
    "        os.makedirs(\"scraped\")\n",
    "    \n",
    "    # Načtení CSV souboru\n",
    "    logger.info(\"Načítání CSV souboru...\")\n",
    "    df = pd.read_csv(\"50-100_url.csv\")\n",
    "    \n",
    "    # Kontrola, zda soubor obsahuje sloupec s URL adresami\n",
    "    if 'url' not in df.columns:\n",
    "        # Pokud sloupec nemá název 'url', předpokládáme, že první sloupec obsahuje URL adresy\n",
    "        df.columns = ['url'] + list(df.columns)[1:]\n",
    "    \n",
    "    # Získání seznamu URL adres\n",
    "    urls = df['url'].tolist()\n",
    "    \n",
    "    # Inicializace seznamů pro výsledky\n",
    "    results = []\n",
    "    \n",
    "    # Paralelní zpracování URL adres\n",
    "    logger.info(\"Začínám scrapování...\")\n",
    "    \n",
    "    # Určení optimálního počtu workerů (obvykle 2-4x počet CPU jader)\n",
    "    max_workers = min(32, os.cpu_count() * 4)\n",
    "    \n",
    "    # Zpracování po dávkách pro lepší správu paměti\n",
    "    batch_size = 1000\n",
    "    total_batches = (len(urls) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(urls), batch_size):\n",
    "        batch_urls = urls[i:i+batch_size]\n",
    "        batch_results = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Použití tqdm pro zobrazení průběhu\n",
    "            futures = {executor.submit(scrape_url, url): url for url in batch_urls}\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(batch_urls), desc=f\"Dávka {i//batch_size + 1}/{total_batches}\"):\n",
    "                batch_results.append(future.result())\n",
    "        \n",
    "        # Přidání výsledků dávky do celkových výsledků\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # Průběžné ukládání výsledků (pro případ selhání)\n",
    "        if (i + batch_size) % (batch_size * 10) == 0 or (i + batch_size) >= len(urls):\n",
    "            temp_df = pd.DataFrame(results, columns=['url', 'xpath_result', 'page_type'])\n",
    "            temp_df.to_csv(f\"scraped/temp_results_{i}.csv\", index=False)\n",
    "            logger.info(f\"Průběžné výsledky uloženy do scraped/temp_results_{i}.csv\")\n",
    "    \n",
    "    # Vytvoření DataFrame z výsledků\n",
    "    result_df = pd.DataFrame(results, columns=['url', 'xpath_result', 'page_type'])\n",
    "    \n",
    "    # Sloučení s původním DataFrame\n",
    "    final_df = pd.merge(df, result_df, on='url', how='left')\n",
    "    \n",
    "    # Uložení výsledků do CSV souboru\n",
    "    output_path = \"scraped/scraped_results.csv\"\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Odstranění dočasných souborů\n",
    "    for i in range(0, len(urls), batch_size * 10):\n",
    "        temp_file = f\"scraped/temp_results_{i}.csv\"\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Scrapování dokončeno. Výsledky uloženy do {output_path}\")\n",
    "    logger.info(f\"Celkový čas: {end_time - start_time:.2f} sekund\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5cdee1-d7fa-450c-8c51-3639c58200d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 10:16:56,880 - INFO - Načítání CSV souboru...\n",
      "Dávka 1/413: 100%|██████████| 500/500 [00:00<00:00, 725.73it/s]\n",
      "Dávka 2/413: 100%|██████████| 500/500 [00:00<00:00, 6673.05it/s]\n",
      "Dávka 3/413: 100%|██████████| 500/500 [00:00<00:00, 6709.51it/s]\n",
      "Dávka 4/413: 100%|██████████| 500/500 [00:00<00:00, 5239.19it/s]\n",
      "Dávka 5/413: 100%|██████████| 500/500 [00:24<00:00, 20.39it/s]\n",
      "Dávka 6/413: 100%|██████████| 500/500 [00:24<00:00, 20.30it/s]\n",
      "Dávka 7/413: 100%|██████████| 500/500 [00:24<00:00, 20.65it/s]\n",
      "Dávka 8/413: 100%|██████████| 500/500 [00:25<00:00, 19.58it/s]\n",
      "Dávka 9/413: 100%|██████████| 500/500 [00:25<00:00, 19.84it/s]\n",
      "Dávka 10/413: 100%|██████████| 500/500 [00:24<00:00, 20.27it/s]\n",
      "2025-04-23 10:19:27,762 - INFO - Průběžné výsledky uloženy do scraped/temp_results_4500.csv\n",
      "Dávka 11/413: 100%|██████████| 500/500 [00:23<00:00, 20.87it/s]\n",
      "Dávka 12/413: 100%|██████████| 500/500 [00:25<00:00, 19.44it/s]\n",
      "Dávka 13/413: 100%|██████████| 500/500 [00:28<00:00, 17.44it/s]\n",
      "Dávka 14/413: 100%|██████████| 500/500 [00:25<00:00, 19.77it/s]\n",
      "Dávka 15/413: 100%|██████████| 500/500 [00:25<00:00, 19.76it/s]\n",
      "Dávka 16/413: 100%|██████████| 500/500 [00:25<00:00, 19.90it/s]\n",
      "Dávka 17/413: 100%|██████████| 500/500 [00:25<00:00, 19.83it/s]\n",
      "Dávka 18/413: 100%|██████████| 500/500 [00:25<00:00, 19.60it/s]\n",
      "Dávka 19/413: 100%|██████████| 500/500 [00:25<00:00, 19.53it/s]\n",
      "Dávka 20/413: 100%|██████████| 500/500 [00:25<00:00, 19.30it/s]\n",
      "2025-04-23 10:23:45,216 - INFO - Průběžné výsledky uloženy do scraped/temp_results_9500.csv\n",
      "Dávka 21/413: 100%|██████████| 500/500 [00:26<00:00, 19.07it/s]\n",
      "Dávka 22/413: 100%|██████████| 500/500 [00:27<00:00, 18.38it/s]\n",
      "Dávka 23/413: 100%|██████████| 500/500 [00:27<00:00, 18.37it/s]\n",
      "Dávka 24/413: 100%|██████████| 500/500 [00:29<00:00, 16.69it/s]\n",
      "Dávka 25/413: 100%|██████████| 500/500 [00:30<00:00, 16.21it/s]\n",
      "Dávka 26/413: 100%|██████████| 500/500 [00:29<00:00, 16.83it/s]\n",
      "Dávka 27/413: 100%|██████████| 500/500 [00:30<00:00, 16.53it/s]\n",
      "Dávka 28/413: 100%|██████████| 500/500 [00:31<00:00, 15.94it/s]\n",
      "Dávka 29/413: 100%|██████████| 500/500 [00:32<00:00, 15.16it/s]\n",
      "Dávka 30/413: 100%|██████████| 500/500 [00:33<00:00, 14.96it/s]\n",
      "2025-04-23 10:28:45,866 - INFO - Průběžné výsledky uloženy do scraped/temp_results_14500.csv\n",
      "Dávka 31/413: 100%|██████████| 500/500 [00:36<00:00, 13.82it/s]\n",
      "Dávka 32/413: 100%|██████████| 500/500 [00:36<00:00, 13.73it/s]\n",
      "Dávka 33/413: 100%|██████████| 500/500 [00:41<00:00, 12.03it/s]\n",
      "Dávka 34/413: 100%|██████████| 500/500 [00:40<00:00, 12.22it/s]\n",
      "Dávka 35/413: 100%|██████████| 500/500 [00:41<00:00, 11.92it/s]\n",
      "Dávka 36/413: 100%|██████████| 500/500 [00:55<00:00,  8.97it/s]\n",
      "Dávka 37/413: 100%|██████████| 500/500 [00:51<00:00,  9.64it/s]\n",
      "Dávka 38/413: 100%|██████████| 500/500 [00:51<00:00,  9.70it/s]\n",
      "Dávka 39/413: 100%|██████████| 500/500 [00:57<00:00,  8.66it/s]\n",
      "Dávka 40/413: 100%|██████████| 500/500 [01:04<00:00,  7.71it/s]\n",
      "2025-04-23 10:36:46,393 - INFO - Průběžné výsledky uloženy do scraped/temp_results_19500.csv\n",
      "Dávka 41/413: 100%|██████████| 500/500 [01:04<00:00,  7.74it/s]\n",
      "Dávka 42/413: 100%|██████████| 500/500 [01:07<00:00,  7.37it/s]\n",
      "Dávka 43/413: 100%|██████████| 500/500 [01:07<00:00,  7.40it/s]\n",
      "Dávka 44/413: 100%|██████████| 500/500 [01:21<00:00,  6.10it/s]\n",
      "Dávka 45/413: 100%|██████████| 500/500 [01:27<00:00,  5.72it/s]\n",
      "Dávka 46/413: 100%|██████████| 500/500 [01:23<00:00,  5.97it/s]\n",
      "Dávka 47/413: 100%|██████████| 500/500 [01:26<00:00,  5.76it/s]\n",
      "Dávka 48/413: 100%|██████████| 500/500 [01:36<00:00,  5.18it/s]\n",
      "Dávka 49/413: 100%|██████████| 500/500 [01:39<00:00,  5.03it/s]\n",
      "Dávka 50/413: 100%|██████████| 500/500 [01:43<00:00,  4.83it/s]\n",
      "2025-04-23 10:50:47,924 - INFO - Průběžné výsledky uloženy do scraped/temp_results_24500.csv\n",
      "Dávka 51/413:  93%|█████████▎| 464/500 [01:41<00:07,  5.07it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import html\n",
    "import concurrent.futures\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "# Nastavení loggeru\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cesta pro ukládání cache\n",
    "CACHE_DIR = \"script_cache\"\n",
    "\n",
    "def create_cache_key(url):\n",
    "    \"\"\"Vytvoří hashovaný klíč z URL\"\"\"\n",
    "    return hashlib.md5(url.encode('utf-8')).hexdigest()\n",
    "\n",
    "def load_cache(url):\n",
    "    \"\"\"Načte cached data pro danou URL\"\"\"\n",
    "    try:\n",
    "        cache_path = os.path.join(CACHE_DIR, f\"{create_cache_key(url)}.json\")\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                cache_data = json.load(f)\n",
    "                # Kontrola stáří cache (např. max 1 hodina)\n",
    "                if time.time() - cache_data['timestamp'] < 3600:\n",
    "                    return cache_data['data']\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Chyba při načítání cache: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_cache(url, data):\n",
    "    \"\"\"Uloží data do cache\"\"\"\n",
    "    try:\n",
    "        # Vytvoření složky pro cache, pokud neexistuje\n",
    "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "        \n",
    "        cache_path = os.path.join(CACHE_DIR, f\"{create_cache_key(url)}.json\")\n",
    "        cache_data = {\n",
    "            'timestamp': time.time(),\n",
    "            'data': data\n",
    "        }\n",
    "        \n",
    "        with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cache_data, f)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Chyba při ukládání cache: {e}\")\n",
    "\n",
    "def clean_old_cache(max_age=86400):  # Výchozí max stáří 24h\n",
    "    \"\"\"Vyčistí staré cache soubory\"\"\"\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        for filename in os.listdir(CACHE_DIR):\n",
    "            file_path = os.path.join(CACHE_DIR, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    cache_data = json.load(f)\n",
    "                    if current_time - cache_data['timestamp'] > max_age:\n",
    "                        os.remove(file_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Chyba při čištění cache: {e}\")\n",
    "\n",
    "# Funkce pro scrapování jedné URL adresy s robustnějším error handlingem\n",
    "def scrape_url(url):\n",
    "    try:\n",
    "        # Nejprve zkusíme načíst z cache\n",
    "        cached_result = load_cache(url)\n",
    "        if cached_result:\n",
    "            return cached_result\n",
    "\n",
    "        # Přidání timeout a exponenciálního backoffu\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Přidání malé prodlevy mezi requesty\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                response = requests.get(url, headers=headers, timeout=10)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    return url, None, None\n",
    "                \n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(f\"Chyba při opakovaném pokusu o {url}: {str(e)}\")\n",
    "                    return url, None, None\n",
    "                \n",
    "                # Exponenciální čekání mezi pokusy\n",
    "                time.sleep(2 ** attempt)\n",
    "        \n",
    "        # Parsování HTML\n",
    "        tree = html.fromstring(response.content)\n",
    "        \n",
    "        # Extrakce dat podle prvního XPath\n",
    "        xpath_result_1 = tree.xpath(\"//span[@itemprop='itemListElement']/a/span[@itemprop='name']/text()\")\n",
    "        result_1 = xpath_result_1[0] if xpath_result_1 else None\n",
    "        \n",
    "        # Extrakce dat podle druhého XPath (regex)\n",
    "        page_type = None\n",
    "        match = re.search(r'\"pageType\"\\s*:\\s*\"(.*?)\"', response.text)\n",
    "        if match:\n",
    "            page_type = match.group(1)\n",
    "        \n",
    "        # Uložení výsledku do cache\n",
    "        result = (url, result_1, page_type)\n",
    "        save_cache(url, result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Neočekávaná chyba při scrapování {url}: {str(e)}\")\n",
    "        return url, None, None\n",
    "\n",
    "# Hlavní funkce\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Čištění staré cache před začátkem\n",
    "    clean_old_cache()\n",
    "    \n",
    "    # Vytvoření složky pro výstup, pokud neexistuje\n",
    "    if not os.path.exists(\"scraped\"):\n",
    "        os.makedirs(\"scraped\")\n",
    "    \n",
    "    # Načtení CSV souboru\n",
    "    logger.info(\"Načítání CSV souboru...\")\n",
    "    df = pd.read_csv(\"50-100_url.csv\")\n",
    "    \n",
    "    # Kontrola, zda soubor obsahuje sloupec s URL adresami\n",
    "    if 'url' not in df.columns:\n",
    "        # Pokud sloupec nemá název 'url', předpokládáme, že první sloupec obsahuje URL adresy\n",
    "        df.columns = ['url'] + list(df.columns)[1:]\n",
    "    \n",
    "    # Získání seznamu URL adres\n",
    "    urls = df['url'].tolist()\n",
    "    \n",
    "    # Inicializace seznamů pro výsledky\n",
    "    results = []\n",
    "    \n",
    "    # Určení optimálního počtu workerů (sníženo pro lepší kontrolu zdrojů)\n",
    "    max_workers = min(16, os.cpu_count() * 2)\n",
    "    \n",
    "    # Zpracování po dávkách pro lepší správu paměti\n",
    "    batch_size = 500  # Sníženo pro menší zátěž\n",
    "    total_batches = (len(urls) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(urls), batch_size):\n",
    "        # Uvolnění paměti před každou dávkou\n",
    "        gc.collect()\n",
    "        \n",
    "        batch_urls = urls[i:i+batch_size]\n",
    "        batch_results = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Použití tqdm pro zobrazení průběhu\n",
    "            futures = {executor.submit(scrape_url, url): url for url in batch_urls}\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(batch_urls), desc=f\"Dávka {i//batch_size + 1}/{total_batches}\"):\n",
    "                batch_results.append(future.result())\n",
    "        \n",
    "        # Přidání výsledků dávky do celkových výsledků\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # Průběžné ukládání výsledků (pro případ selhání)\n",
    "        if (i + batch_size) % (batch_size * 10) == 0 or (i + batch_size) >= len(urls):\n",
    "            temp_df = pd.DataFrame(results, columns=['url', 'xpath_result', 'page_type'])\n",
    "            temp_df.to_csv(f\"scraped/temp_results_{i}.csv\", index=False)\n",
    "            logger.info(f\"Průběžné výsledky uloženy do scraped/temp_results_{i}.csv\")\n",
    "    \n",
    "    # Vytvoření DataFrame z výsledků\n",
    "    result_df = pd.DataFrame(results, columns=['url', 'xpath_result', 'page_type'])\n",
    "    \n",
    "    # Sloučení s původním DataFrame\n",
    "    final_df = pd.merge(df, result_df, on='url', how='left')\n",
    "    \n",
    "    # Uložení výsledků do CSV souboru\n",
    "    output_path = \"scraped/scraped_results.csv\"\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Odstranění dočasných souborů\n",
    "    for i in range(0, len(urls), batch_size * 10):\n",
    "        temp_file = f\"scraped/temp_results_{i}.csv\"\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Scrapování dokončeno. Výsledky uloženy do {output_path}\")\n",
    "    logger.info(f\"Celkový čas: {end_time - start_time:.2f} sekund\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd60294-1b79-4cb7-b9d1-df00a911a169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5e399-36e0-4602-8a3d-4de320eaea8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
